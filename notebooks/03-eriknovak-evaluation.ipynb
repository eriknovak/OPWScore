{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# enable automatic reloading of the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from src.utils.metrics import get_pearson_r, get_spearman_r, get_kendall_tau, get_t_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMT18 Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pairs = [\n",
    "    \"cs-en\",\n",
    "    \"de-en\",\n",
    "    \"et-en\",\n",
    "    \"fi-en\",\n",
    "    \"ru-en\",\n",
    "    \"tr-en\",\n",
    "    \"zh-en\",\n",
    "    \"en-cs\",\n",
    "    \"en-de\",\n",
    "    \"en-et\",\n",
    "    \"en-fi\",\n",
    "    \"en-ru\",\n",
    "    \"en-tr\",\n",
    "    \"en-zh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_1 = \"../archive/results_emd_idf_20230119/wmt18/\"\n",
    "model_path_2 = \"../archive/results_emd_uniform_20230118/wmt18/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(lang_pair, model_path):\n",
    "    with open(os.path.join(model_path, f\"scores.{lang_pair}.json\"), \"r\", encoding=\"utf8\") as f:\n",
    "        return [k[\"system_score\"] for k in json.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_t_test(lang_pair):\n",
    "    model_scores_1 = get_model_scores(lang_pair, model_path_1)\n",
    "    model_scores_2 = get_model_scores(lang_pair, model_path_2)\n",
    "    return get_t_test(model_scores_1, model_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs-en Ttest_indResult(statistic=19.956666645232744, pvalue=1.0918613945700628e-87)\n",
      "de-en Ttest_indResult(statistic=29.155633739202027, pvalue=9.791157357385664e-186)\n",
      "et-en Ttest_indResult(statistic=28.32481411960971, pvalue=5.973909479501825e-175)\n",
      "fi-en Ttest_indResult(statistic=34.77894454532147, pvalue=3.431230205311238e-260)\n",
      "ru-en Ttest_indResult(statistic=24.52664848466707, pvalue=1.7414777289110105e-131)\n",
      "tr-en Ttest_indResult(statistic=23.100246231581046, pvalue=6.31005660365584e-117)\n",
      "zh-en Ttest_indResult(statistic=31.834582191045445, pvalue=1.886804436914362e-220)\n",
      "en-cs Ttest_indResult(statistic=13.547417659873181, pvalue=1.394053540207454e-41)\n",
      "en-de Ttest_indResult(statistic=14.685160394286475, pvalue=1.3056401502013133e-48)\n",
      "Error, unable to compare scores\n",
      "en-fi Ttest_indResult(statistic=17.8491726665699, pvalue=1.2400036414985596e-70)\n",
      "en-ru Ttest_indResult(statistic=20.667966141251274, pvalue=2.3286719128672358e-94)\n",
      "en-tr Ttest_indResult(statistic=8.072521166479888, pvalue=8.076494032474833e-16)\n",
      "Error, unable to compare scores\n"
     ]
    }
   ],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    try:\n",
    "        print(lang_pair, calculate_t_test(lang_pair))\n",
    "    except:\n",
    "        print(\"Error, unable to compare scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.WMT20 import WMT20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs-en',\n",
       " 'de-en',\n",
       " 'iu-en',\n",
       " 'ja-en',\n",
       " 'km-en',\n",
       " 'pl-en',\n",
       " 'ps-en',\n",
       " 'ru-en',\n",
       " 'ta-en',\n",
       " 'zh-en',\n",
       " 'en-cs',\n",
       " 'en-de',\n",
       " 'en-iu',\n",
       " 'en-ja',\n",
       " 'en-km',\n",
       " 'en-pl',\n",
       " 'en-ps',\n",
       " 'en-ru',\n",
       " 'en-ta',\n",
       " 'en-zh',\n",
       " 'de-fr',\n",
       " 'fr-de']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WMT20.supported_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = WMT20(WMT20.supported_languages[0], batch_size=16).setup()\n",
    "dataloader = data.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/erikn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "wmt20-comet-da is already in cache.\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "# prepare the models\n",
    "model_bleu = load(\"bleu\")\n",
    "model_meteor = load(\"meteor\")\n",
    "model_rouge = load(\"rouge\")\n",
    "model_bertscore = load(\"bertscore\")\n",
    "model_comet = load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu_1_score(predictions, references, sources=None, lang=None):\n",
    "    def _process_single_example(pred, ref):\n",
    "        try:\n",
    "            results = model_bleu.compute(predictions=[pred], references=[[ref]])\n",
    "        except:\n",
    "            results = {\"precisions\": [0, 0, 0, 0]}\n",
    "        return results[\"precisions\"][0]\n",
    "    return [_process_single_example(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "\n",
    "\n",
    "def get_bleu_2_score(predictions, references, sources=None, lang=None):\n",
    "    def _process_single_example(pred, ref):\n",
    "        try:\n",
    "            results = model_bleu.compute(predictions=[pred], references=[[ref]])\n",
    "        except:\n",
    "            results = {\"precisions\": [0, 0, 0, 0]}\n",
    "        return results[\"precisions\"][1]\n",
    "    return [_process_single_example(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "\n",
    "\n",
    "def get_bleu_3_score(predictions, references, sources=None, lang=None):\n",
    "    def _process_single_example(pred, ref):\n",
    "        try:\n",
    "            results = model_bleu.compute(predictions=[pred], references=[[ref]])\n",
    "        except:\n",
    "            results = {\"precisions\": [0, 0, 0, 0]}\n",
    "        return results[\"precisions\"][2]\n",
    "    return [_process_single_example(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "\n",
    "\n",
    "def get_bleu_4_score(predictions, references, sources=None, lang=None):\n",
    "    def _process_single_example(pred, ref):\n",
    "        try:\n",
    "            results = model_bleu.compute(predictions=[pred], references=[[ref]])\n",
    "        except:\n",
    "            results = {\"precisions\": [0, 0, 0, 0]}\n",
    "        return results[\"precisions\"][3]\n",
    "    return [_process_single_example(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "\n",
    "\n",
    "def get_meteor_score(predictions, references, sources=None, lang=None):\n",
    "    results = model_meteor.compute(predictions=predictions, references=references)\n",
    "    return results[\"meteor\"]\n",
    "\n",
    "\n",
    "def get_rougel_score(predictions, references, sources=None, lang=None):\n",
    "    def _process_single_example(pred, ref):\n",
    "        try:\n",
    "            results = model_rouge.compute(predictions=[pred], references=[ref])\n",
    "        except:\n",
    "            results = {\"rougeL\": 0}\n",
    "        return results[\"rougeL\"]\n",
    "    return [_process_single_example(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "\n",
    "\n",
    "def get_bertscore_score(predictions, references, sources=None, lang=\"en\"):\n",
    "    results = model_bertscore.compute(predictions=predictions, references=references, lang=lang)\n",
    "    return results[\"f1\"]\n",
    "\n",
    "\n",
    "def get_comet_score(predictions, references, sources, lang=None):\n",
    "    results = model_comet.compute(predictions=predictions, references=references, sources=sources, gpus=1)\n",
    "    return results[\"scores\"]\n",
    "\n",
    "\n",
    "models = [\n",
    "    {\"id\": \"BLEU-1\", \"model\": get_bleu_1_score},\n",
    "    {\"id\": \"BLEU-2\", \"model\": get_bleu_2_score},\n",
    "    {\"id\": \"BLEU-3\", \"model\": get_bleu_3_score},\n",
    "    {\"id\": \"BLEU-4\", \"model\": get_bleu_4_score},\n",
    "    {\"id\": \"METEOR\", \"model\": get_meteor_score},\n",
    "    {\"id\": \"ROUGE-L\", \"model\": get_rougel_score},\n",
    "    # {\"id\": \"BERTScore\", \"model\": get_bertscore_score},\n",
    "    # {\"id\": \"COMET\", \"model\": get_comet_score},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n",
      "Using default tokenizer.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'bert_score' has no attribute 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m system_scores \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model_scores \u001b[39m=\u001b[39m model[\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m](\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         predictions\u001b[39m=\u001b[39;49mdata[\u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         references\u001b[39m=\u001b[39;49mdata[\u001b[39m\"\u001b[39;49m\u001b[39mreference\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         sources\u001b[39m=\u001b[39;49mdata[\u001b[39m\"\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         lang\u001b[39m=\u001b[39;49mlanguage,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     system_scores[model[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m model_scores\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m])):\n",
      "\u001b[1;32m/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb Cell 16\u001b[0m in \u001b[0;36mget_bertscore_score\u001b[0;34m(predictions, references, sources, lang)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_bertscore_score\u001b[39m(predictions, references, sources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     results \u001b[39m=\u001b[39m model_bertscore\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mreferences, lang\u001b[39m=\u001b[39;49mlang)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikn/Documents/code/experiments/model-Seq-LM-EMD/notebooks/03-eriknovak-evaluation.ipynb#X60sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Programs/anaconda3/envs/seq-lm-emd/lib/python3.8/site-packages/evaluate/module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bertscore/cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b/bertscore.py:155\u001b[0m, in \u001b[0;36mBERTScore._compute\u001b[0;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     idf_sents \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m get_hash \u001b[39m=\u001b[39m bert_score\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39mget_hash\n\u001b[1;32m    156\u001b[0m scorer \u001b[39m=\u001b[39m bert_score\u001b[39m.\u001b[39mBERTScorer\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(bert_score\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m0.3.10\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'bert_score' has no attribute 'utils'"
     ]
    }
   ],
   "source": [
    "# load the datasets\n",
    "lang_pair = WMT20.supported_languages[0]\n",
    "dataset = WMT20(lang_pair, batch_size=16)\n",
    "dataloader = dataset.setup().test_dataloader()\n",
    "language = lang_pair.split(\"-\")[1]\n",
    "# calculate the scores\n",
    "scores = []\n",
    "\n",
    "for data in dataloader:\n",
    "    src_model_ids = data[\"model_id\"]\n",
    "    system_scores = {}\n",
    "    for model in models:\n",
    "        model_scores = model[\"model\"](\n",
    "            predictions=data[\"system\"],\n",
    "            references=data[\"reference\"],\n",
    "            sources=data[\"source\"],\n",
    "            lang=language,\n",
    "        )\n",
    "        system_scores[model[\"id\"]] = model_scores\n",
    "\n",
    "    for idx in range(len(data[\"score\"])):\n",
    "        system_score = { key: vals[idx] for key, vals in system_scores.items() }\n",
    "\n",
    "        scores.append(\n",
    "            {\n",
    "                **system_score,\n",
    "                \"model_id\": src_model_ids[idx],\n",
    "                \"human_score\": data[\"score\"][idx].item(),\n",
    "            }\n",
    "        )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU-1': [0.7857142857142857,\n",
       "  0.75,\n",
       "  0.7479674796747967,\n",
       "  0.7819548872180451,\n",
       "  0.6574074074074074,\n",
       "  0.6923076923076923,\n",
       "  0.691358024691358,\n",
       "  0.5384615384615384,\n",
       "  0.6764705882352942,\n",
       "  0.696969696969697,\n",
       "  0.725,\n",
       "  0.8148148148148148,\n",
       "  0.6363636363636364,\n",
       "  0.6153846153846154,\n",
       "  0.5483870967741935,\n",
       "  0.6891891891891891],\n",
       " 'BLEU-2': [0.38461538461538464,\n",
       "  0.5333333333333333,\n",
       "  0.4262295081967213,\n",
       "  0.4696969696969697,\n",
       "  0.3364485981308411,\n",
       "  0.5324675324675324,\n",
       "  0.3416149068322981,\n",
       "  0.25,\n",
       "  0.417910447761194,\n",
       "  0.3469387755102041,\n",
       "  0.4430379746835443,\n",
       "  0.5283018867924528,\n",
       "  0.32653061224489793,\n",
       "  0.4166666666666667,\n",
       "  0.21311475409836064,\n",
       "  0.3835616438356164],\n",
       " 'BLEU-3': [0.25,\n",
       "  0.35714285714285715,\n",
       "  0.256198347107438,\n",
       "  0.31297709923664124,\n",
       "  0.20754716981132076,\n",
       "  0.4342105263157895,\n",
       "  0.1875,\n",
       "  0.09090909090909091,\n",
       "  0.25757575757575757,\n",
       "  0.18556701030927836,\n",
       "  0.28205128205128205,\n",
       "  0.28846153846153844,\n",
       "  0.16494845360824742,\n",
       "  0.36363636363636365,\n",
       "  0.1,\n",
       "  0.2222222222222222],\n",
       " 'BLEU-4': [0.09090909090909091,\n",
       "  0.23076923076923078,\n",
       "  0.14166666666666666,\n",
       "  0.17692307692307693,\n",
       "  0.12380952380952381,\n",
       "  0.38666666666666666,\n",
       "  0.1069182389937107,\n",
       "  0.0,\n",
       "  0.15384615384615385,\n",
       "  0.11458333333333333,\n",
       "  0.18181818181818182,\n",
       "  0.13725490196078433,\n",
       "  0.0625,\n",
       "  0.3,\n",
       "  0.05084745762711865,\n",
       "  0.15492957746478872],\n",
       " 'METEOR': [0.7762397464578674,\n",
       "  0.7613856163179182,\n",
       "  0.5360570074512381,\n",
       "  0.6641104382938883,\n",
       "  0.5273942232360972,\n",
       "  0.5853699504493155,\n",
       "  0.5265543326216594,\n",
       "  0.8642857142857141,\n",
       "  0.5469749148151584,\n",
       "  0.39091238630080205,\n",
       "  0.6537319552035906,\n",
       "  0.5496478873239437,\n",
       "  0.4757684964200477,\n",
       "  0.6485954553587404,\n",
       "  0.3237171717171717,\n",
       "  0.5190010419430121],\n",
       " 'ROUGE-L': [0.7272727272727272,\n",
       "  0.7741935483870969,\n",
       "  0.5145228215767634,\n",
       "  0.6271186440677966,\n",
       "  0.5641025641025641,\n",
       "  0.6712328767123288,\n",
       "  0.5387205387205387,\n",
       "  0.6666666666666667,\n",
       "  0.6190476190476191,\n",
       "  0.5050505050505051,\n",
       "  0.6490066225165563,\n",
       "  0.5740740740740742,\n",
       "  0.5060240963855421,\n",
       "  0.5714285714285715,\n",
       "  0.41025641025641024,\n",
       "  0.5755395683453237]}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('seq-lm-emd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006d38af847dd7867da3438f390d9a22fb50044c28d4e0569a79cb9d6728a4c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
