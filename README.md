# OPWScore

This project contains the code for running the OPWScore experiments.

## ‚òëÔ∏è Requirements

Before starting the project make sure these requirements are available:

- [conda][conda]. For setting up your research environment and python dependencies.
- [dvc][dvc]. For versioning your data.
- [git][git]. For versioning your code.

## üõ†Ô∏è Setup

### Create a python environment

First create the virtual environment where all the modules will be stored.

#### Using virtualenv

Using the `virtualenv` command, run the following commands:

```bash
# install the virtual env command
pip install virtualenv

# create a new virtual environment
virtualenv -p python ./.venv

# activate the environment (UNIX)
./.venv/bin/activate

# activate the environment (WINDOWS)
./.venv/Scripts/activate

# deactivate the environment (UNIX & WINDOWS)
deactivate
```

#### Using conda

Install [conda][conda], a program for creating python virtual environments. Then run the following commands:

```bash
# create a new virtual environment
conda create --name opwscore python=3.8 pip

# activate the environment
conda activate opwscore

# deactivate the environment
deactivate
```

### Install

To install the requirements run:

```bash
pip install -e .
```

## üóÉÔ∏è Data

The data used in the experiments are examples from the WMT17, WMT18 and WMT20
metric evaluation data sets.

The data sets are taken from the COMET metric page. Download the files and store them as stated in the table.

| Data set | Folder Save Path | Link                                                                                                 |
| -------- | ---------------- | ---------------------------------------------------------------------------------------------------- |
| WMT17    | `data/raw/wmt17` | [Download](https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2017-da.csv.tar.gz) |
| WMT18    | `data/raw/wmt18` | [Download](https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2018-da.csv.tar.gz) |
| WMT20    | `data/raw/wmt20` | [Download](https://unbabel-experimental-data-sets.s3.eu-west-1.amazonaws.com/wmt/2020-da.csv.tar.gz) |

## ‚öóÔ∏è Experiments

To run the experiments, run the folowing commands:

```bash
# calculate the IDF weights
python scripts/models/compute_weights.py en,cs,de,fi,ru,tr,zh

# run the adequacy experiments on the selected languages and data sets
python scripts/models/performance_test.py en,cs,de,fi,ru,tr,zh wmt18,wmt20

# calculate the model's adequacy performance scores on the provided data sets
python scripts/models/performance_eval.py wmt18,wmt20

# run the fluency experiments on the selected data sets
python scripts/models/fluency_test.py wmt18

# calculate the model's fluency performance scores on the provided data sets
python scripts/models/fluency_eval.py wmt18
```

### ü¶â Using DVC

An alternative way of running the whole experiment is by using [DVC][dvc]. To do this,
simply run:

```bash
dvc exp run
```

This command will read the `dvc.yaml` file and execute the stages accordingly, taking
any dependencies into consideration. **NOTE: This will only run the experiments on the WMT18 data sets.**

Afterwards, we can compare the performance of the models by running:

```bash
dvc exp show
```

To save the best performance parameters run:

```bash
# [exp-id] is the ID of the experiment that yielded the best performance
dvc exp apply [exp-id]
```

### Results

The `results` folder contain the experimental results.

## üìö Papers

In case you use any of the components for your research, please refer to
(and cite) the papers:

TODO

## üì£ Acknowledgments

This work is developed by [Department of Artificial Intelligence][ailab] at [Jozef Stefan Institute][ijs].

This work was supported by the Slovenian Research Agency, and the European Union's Horizon 2020 project Humane AI Net [H2020-ICT-952026] and the Horizon Europe project enRichMyData [HE-101070284].

[python]: https://www.python.org/
[conda]: https://www.anaconda.com/
[git]: https://git-scm.com/
[dvc]: https://dvc.org/
[ailab]: http://ailab.ijs.si/
[ijs]: https://www.ijs.si/
